<!-- 将推理和行动结合， -->
<!-- 2023 thu  -->
# ReAct: Synergizing Reasoning and Acting in Language Models
协同推理与行动

尽管大语言模型（LLMs）在语言理解和交互式决策任务中已展现出卓越性能，但其**推理能力**（例如思维链提示，chain-of-thought prompting）与**行动能力**（例如生成行动计划）的研究长期以来基本是割裂进行的。本文提出一种名为 ReAct 的方法，让 LLM 交替生成推理轨迹与任务相关的具体动作，从而实现二者的深度协同

推理轨迹帮助模型制定、跟踪并动态更新行动计划，同时处理异常情况；
具体动作则使模型能够与外部环境（如知识库或交互系统）交互，获取额外信息以支持推理。

compared to baseline 性能更优，而且生成的轨迹更易于人类理解，增强了可信度
在问答任务（HotpotQA）和事实验证任务（Fever）上，ReAct 通过调用一个简单的 Wikipedia API，有效缓解了传统思维链中常见的幻觉（hallucination）问题，并生成了类似人类的、可解释的任务解决过程。
在两个交互式决策基准（ALFWorld 和 WebShop）上，ReAct 仅凭1–2 个示例进行上下文学习（in-context learning），就分别以 34% 和 10% 的绝对成功率优势，超越了模仿学习与强化学习方法
[baseline 数据支持]


## introduction
人类智能的一个独特特征，是能够**无缝融合任务导向的行动与语言推理**（即“内心言语”，Alderson-Day & Fernyhough, 2015）。理论认为，这种能力在人类认知中扮演着关键角色：它支持自我调节与策略规划（Vygotsky, 1987；Luria, 1965；Fernyhough, 2010），并有助于维持工作记忆（Baddeley, 1992）。

在任意两个具体动作之间，我们可能会用语言进行推理
做了什么事/发生什么情况/需要额外信息/ -> 言语推理 ->行动

exp:
追踪进度：“现在所有食材都切好了，我该把水烧开了”；
处理异常或调整计划：“我没有盐了，那就用酱油和胡椒代替吧”；
意识到需要外部信息：“面团该怎么准备？我上网查一下”。

近期研究暗示，这种推理与行动的结合也可能应用于自主系统
但现在的CoT推理是一个静态黑箱：模型仅依赖其内部表征生成思维，未与外部世界建立联系，因此难以进行反应式推理或更新知识，容易导致事实幻觉（hallucination） 和推理过程中的错误传播（如图1(1b)所示）

![image 1](image.png)

另一方面，近期工作探索了使用预训练语言模型在交互环境中进行规划与行动（Ahnet al., 2022；Nakano et al., 2021；Yao et al., 2020；Huang et al., 2022a），重点是利用语言先验来预测动作。
除了这类与少量积木交互的简单具身任务外，尚无研究系统探索如何在通用任务求解中协同结合推理与行动，也未验证这种结合是否比单独使用推理或行动更具系统性优势。

在本文中，我们提出了 ReAct（Reason + Act）——一种通用范式，通过语言模型协同推理与行动，以解决多样化的语言推理与决策任务（见图1）。
ReAct 通过**提示 LLM 交替生成与任务相关的言语推理轨迹与具体动作**，使模型能够：

动态推理，以制定、维护并调整高层行动计划（推理以指导行动）；
与外部环境（如 Wikipedia），将新信息融入推理过程（行动以支持推理）。


我们对 ReAct 与当前最先进的基线方法在四个多样化的基准上进行了**实证评估**
问答任务（HotpotQA，Yang 等，2018）
事实验证（Fever，Thorne 等，2018）
文本游戏（ALFWorld，Shridhar 等，2020b）
网页导航（WebShop，Yao 等，2022）

HotpotQA/Fever 
整体表现最优的方案是 ReAct 与 CoT 的结合，因为它能在推理过程中同时利用模型的内部知识与从外部获取的信息。

ALFWorld/WebShop
仅使用一到两个示例（one- or two-shot），ReAct 就显著超越了那些使用 10³ 至 10⁵ 个任务实例训练的模仿学习或强化学习方法，在成功率上分别实现了 **34% 和 10%** 的绝对提升。

推理与行动的结合还显著增强了模型的可解释性、可信度与可诊断性
:区分模型所用信息是来自其内部知识还是外部环境,理解其动作背后的决策依据

**总结：本文的核心贡献如下：**
提出 ReAct：一种新颖的基于提示的范式，首次在语言模型中系统性地协同推理与行动，以解决通用任务；
广泛实验验证：在多个多样化基准上证明，ReAct 在少样本（few-shot） 设置下显著优于仅进行推理或仅生成动作的现有方法；
系统性消融分析：深入探究了在推理任务中引入行动的重要性，以及在交互式任务中引入推理的价值；
分析局限并探索改进路径：指出在 纯提示 范式下，ReAct 对复杂推理与行动行为的支持有限，并通过初步的微调实验表明，引入更多训练数据可进一步提升 ReAct 的能力。

## ReAct：协同推理与行动
智能体与环境交互以完成任务
ct = (o1,a1,...,ot-1,at-1,ot)
上下文  迄今为止的完整交互历史

当从上下文 ct 到动作 at 的映射高度隐式、且需要复杂推理时，学习该策略极具挑战性。
1c 和 2a的失败便是如此

ReAct 的核心思想很简单：我们将智能体的动作空间扩展为 **A` = A ∪ L**
其中 L是语言空间。属于语言空间的动作 
at∈L（我们称之为“思考”或“推理轨迹”）不会对外部环境产生影响，因此不会引发新的观测反馈。

相反，这样的“思考”旨在通过对当前上下文 ct进行推理，整合有用信息
并更新上下文为 c(t+1) = (ct,at)
[每一次针对于现在先思考，然后把思考内容并入上下文]

这类“思考”可以有多种形式：
分解任务目标并制定行动计划（图2b的Act 1；图1d的Thought 1）
注入与任务相关的常识知识（图2b的Act 1）
从观测中提取关键信息（图1d的Thought 2、4）
跟踪任务进度并调整行动计划（图2b的Act 8）
处理异常情况并修正行动策略（图1d的Thought 3）

由于语言空间L是无限的  在这种扩展的动作空间中进行学习非常困难  故
使用一个冻结的大型语言模型，通过少量的上下文示例（few-shot in-context examples）来同时生成领域相关的动作和自由形式的语言思考（如图1(1d)和(2b)所示）。每个上下文示例都是一条由人类完成的、包含动作、思考和环境观测的任务轨迹（见附录C）。

推理为主的任务 ： 交替生成   thoughts and actions  （1（2））
需要大量动作的决策型任务 ： “思考”只需在轨迹中最关键的位置稀疏地出现，因此我们让语言模型自主决定思考与动作的异步发生时机

由于推理与决策能力都被整合到一个大型语言模型中，ReAct 具备以下几个**独特优势**
A 直观且易于设计：设计 ReAct 提示非常简单，因为人工标注者只需在自己采取的动作之上，用自然语言写下自己的思考过程。
B 通用且灵活：得益于灵活的思考空间和自由的思考–动作出现形式，ReAct 适用于多种任务，包括问答、事实验证、文字游戏、网页导航等，这些任务具有截然不同的动作空间和推理需求。
C 高效且鲁棒：ReAct 仅通过1至6个上下文示例就能在新任务实例上表现出强大的泛化能力，在不同领域中持续优于仅含推理或仅含行动的基线方法。
D 与人类对齐且可控制：ReAct 提供了一个可解释的序列决策与推理过程，人类可以轻松检查其推理逻辑和事实正确性。此外，人类还可以通过编辑思考内容，实时干预或修正智能体的行为（如第4节图5所示）。

## 知识密集型推理任务
例如多跳问答（multi-hop question answering）和事实验证（fact verification）
如图1(1d)所示，通过与维基百科API交互，ReAct能够检索信息以支持推理，同时利用推理来决定下一步要检索什么内容，从而展现出推理与行动之间的协同作用。

### setup 
**HotPotQA**一个多跳问答基准数据集，要求模型基于两个或多个维基百科段落进行推理
**FEVER** 一个事实验证基准数据集，其中每条声明都会被标注为 SUPPORTS（支持）、REFUTES（反驳）或 NOT ENOUGH INFO（信息不足），判断依据是是否存在维基百科段落可以验证该声明

在本研究中，我们在“仅问题”（question-only）的设定下处理这两个任务
1. 模型仅接收问题（或声明）作为输入，无法直接访问支持段落，必须依赖其内部知识
2. 或通过与外部环境（例如维基百科）交互来检索所需知识，以支持后续推理。

**动作空间**  我们设计了一个简单的维基百科网页API，包含以下三类动作，以支持交互式信息检索

(1) search[实体]：若该实体的维基百科页面存在，则返回其页面的前5句话；否则，返回维基百科搜索引擎建议的前5个相似实体；
(2) lookup[字符串]：返回当前页面中包含该字符串的下一个句子，模拟浏览器中的“Ctrl+F”查找功能；
(3) finish[答案]：结束当前任务并提交答案。

模拟人类如何与维基百科交互，并迫使模型通过显式的语言推理来引导信息检索过程

### 方法
**ReAct 提示（ReAct Prompting）：**
针对 HotpotQA 和 FEVER，我们分别从训练集中随机选取 6 个和 3 个样本，并人工编写符合 ReAct 格式的推理轨迹（trajectories），作为提示中的少样本示例（few-shot exemplars）

每条轨迹由多个“思考–行动–观察”（thought-action-observation）步骤组成（即“密集思考”，dense thought）
结合了以下类型的思考

问题分解（例如：“我需要先搜索 x，找到 y，然后再找 z”）；
从维基百科观察中提取信息（例如：“x 始于 1844 年”、“该段落未提及 x”）；
常识推理（例如：“x 不是 y，所以 z 一定是……”）；
算术推理（例如：“1844 < 1989”）；
引导搜索重写（例如：“也许我可以改搜/查找 x”）；
综合生成最终答案（例如：“……因此答案是 x”）。

and others

**基线方法（Baselines）：**
1. 标准提示（Standard）： 问题 - 答案  移除 ReAct 轨迹中的所有思考、行动和观察
2. 思维链提示（Chain-of-Thought, CoT）  移除行动和观察，仅保留思考链
   以及 自一致性基线（CoT-SC） 在推理时以温度 0.7 采样 21 条 CoT 路径，并选择出现次数最多的答案，这种方法被证明能持续提升 CoT 的性能
3. 仅行动提示（Act）： 移除 ReAct 轨迹中的所有思考，仅保留行动和观察，大致类似于 WebGPT与互联
   网交互回答问题的方式

**融合内部与外部知识：**
ReAct 所展示的问题解决过程更具事实性和可验证性
CoT 在推理结构上更准确，但容易产生虚构的事实或错误的推理
因此，我们提出将 ReAct 与 CoT-SC 结合
1.ReAct → CoT-SC：当 ReAct 在指定步数内未能返回答案时，回退到 CoT-SC。
2.CoT-SC → ReAct：当 CoT-SC 采样的 n 个答案中，得票最多的答案出现次数少于 n/2 时（即模型内部知识对任务缺乏足够信心），则回退到 ReAct。

**微调（Finetuning）：**
大规模人工标注推理轨迹和行动极具挑战
使用 ReAct（以及其他基线）生成的 3,000 条答案正确的轨迹，对较小的语言模型（如 PaLM-8B/62B）进行微调，使其能够根据输入的问题或声明，直接生成完整的轨迹

### 结果和观察

![alt text](image-1.png)

![alt text](image-2.png)

![alt text](image-3.png)

**ReAct 一直优于仅行动方法（Act）**
table 1  
ReAct 在两项任务上均优于 Act，这体现了推理引导行动的价值，尤其是在最终答案的综合生成方面


**ReAct 与 CoT 的对比**
ReAct 在 FEVER 上优于 CoT（60.9 vs. 56.3），但在 HotpotQA 上略逊于 CoT（27.4 vs. 29.4）

这一差异 ： FEVER 中“支持”（SUPPORTS）与“反驳”（REFUTES）的声明之间可能仅有细微差别（参见附录 D.1），因此通过行动检索准确且最新的知识至关重要

表2 ： 人工标注 ReAct 与 CoT 在 HotpotQA 上的 50 ture 50 false 的原因
分析如下

1. **CoT** 存在严重的“**幻觉**”问题 14%假阳性 56%阴性原因
2. 推理、行动与观察的交错执行提升了 ReAct 的可信度和事实性  但降低了其在组织推理步骤时的灵活性
   我们注意到 ReAct 中存在一种特有错误模式：模型会重复生成之前的思考和行动，无法判断下一步该采取什么行动，从而陷入循环。 ----我们将此归类为“**推理错误**”。
3. 对 **ReAct** 而言，通过**搜索成功**获取信息丰富的知识至关重要
   23% 的错误案例源于“无信息量的搜索”（non-informative search）
   这或许反映了事实性与灵活性之间的权衡


**ReAct + CoT-SC 是提示大语言模型的最佳方法**
如表 1 所示，在 HotpotQA 和 FEVER 上表现最佳的提示方法分别是 ReAct → CoT-SC 和 CoT-SC → ReAct。
它们在所有采样数量下均显著且持续优于纯 CoT-SC
-->合理融合模型内部知识与外部知识对推理任务具有重要价值\


**ReAct 在微调中表现最佳**
使用 PaLM-8B/62B 时，提示 ReAct 的表现是四种方法中最差的，因为仅靠上下文示例难以同时学会推理与行动。
然而，仅用 3,000 条示例进行微调后，ReAct 成为四种方法中表现最好的：

微调后的 PaLM-8B ReAct 模型，性能超过所有 PaLM-62B 的提示方法；
微调后的 PaLM-62B ReAct 模型，性能超过所有 PaLM-540B 的提示方法。


对 Standard 或 CoT 进行微调的效果远不如 ReAct 或 Act（无论对 PaLM-8B 还是 62B）

前者本质上是在教模型记忆（可能虚构的）知识事实，而后者则教会模型如何（推理并）通过行动从维基百科获取信息——这是一种更具泛化能力的知识推理技能。

尽管如此，所有提示方法与领域专用的最先进方法相比仍有显著差距（见表 1）。我们认为，使用更多人工编写的轨迹数据进行微调，可能是充分释放 ReAct 潜力的更优路径。

## 决策任务 （基于语言的交互式决策任务）
ALFWorld 和 WebShop
两个任务都具有复杂的环境，要求智能体在稀疏奖励和长时程交互中有效行动与探索

### ALFWorld
合成的文本式游戏
它包含 6 类任务,智能体需通过文本动作（如 go to coffeetable 1、take paper 2、use desklamp 1）在模拟家居环境中导航并交互，以完成高层目标（如“在台灯下查看纸张”）。

ALFWorld 的一个关键难点在于：智能体需判断常见家居物品的可能位置（如台灯很可能位于书桌、架子或梳妆台上），这使其成为大语言模型（LLMs）利用预训练常识知识的理想测试平台。

为**构造** ReAct 提示，我们从训练集中为每类任务随机标注 3 条轨迹
用于分解目标；跟踪子目标完成情况；确定下一个子目标；利用常识推理物品可能位置及其操作方式。
我们对每类任务通过从 3 条标注轨迹中任选 2 条的所有排列（共 6 种组合）构建 6 个不同提示
Act 提示使用相同的轨迹，但移除所有思考内容。
**基线方法**采用 BUTLER（Shridhar 等，2020b）——一种基于每类任务 105 条专家轨迹训练的模仿学习智能体。

### WebShop
在线购物网站模拟环境，包含 118 万真实商品和 1.2 万条人工指令
WebShop 包含大量结构化与非结构化文本
要求智能体根据用户指令（如“我想要一个带抽屉的床头柜，表面为镍色，价格低于 140 美元”）通过**网页交互**（如搜索“nightstand drawers”、点击“color: modern-nickel-white”、或“返回搜索”）完成商品购买。

我们为 Act **构造**的提示包含搜索、选择商品、选择选项、购买等动作；而 ReAct 的提示则额外加入推理，用于：判断应探索什么内容；何时购买；哪些商品选项与指令相关

**基线方法**
一种基于 1,012 条人工标注轨迹训练的模仿学习（IL） 方法；
一种在此基础上额外使用 10,587 条训练指令进行模仿+强化学习（IL+RL） 的方法。

### 结果

![alt text](image-4.png)

ReAct 在 ALFWorld（表 3）和 WebShop（表 4）上均显著优于 Act。









# Noun explanation && Extensive knowledge 
## 推理能力 & 行动能力

## CoT-SC

## WebGPT





# 思考？


