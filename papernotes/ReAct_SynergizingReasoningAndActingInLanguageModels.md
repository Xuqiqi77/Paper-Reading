<!-- -->
<!-- chinese -->
# ReAct: Synergizing Reasoning and Acting in Language Models
协同推理与行动

尽管大语言模型（LLMs）在语言理解和交互式决策任务中已展现出卓越性能，但其**推理能力**（例如思维链提示，chain-of-thought prompting）与**行动能力**（例如生成行动计划）的研究长期以来基本是割裂进行的。本文提出一种名为 ReAct 的方法，让 LLM 交替生成推理轨迹与任务相关的具体动作，从而实现二者的深度协同

推理轨迹帮助模型制定、跟踪并动态更新行动计划，同时处理异常情况；
具体动作则使模型能够与外部环境（如知识库或交互系统）交互，获取额外信息以支持推理。

compared to baseline 性能更优，而且生成的轨迹更易于人类理解，增强了可信度
在问答任务（HotpotQA）和事实验证任务（Fever）上，ReAct 通过调用一个简单的 Wikipedia API，有效缓解了传统思维链中常见的幻觉（hallucination）问题，并生成了类似人类的、可解释的任务解决过程。
在两个交互式决策基准（ALFWorld 和 WebShop）上，ReAct 仅凭1–2 个示例进行上下文学习（in-context learning），就分别以 34% 和 10% 的绝对成功率优势，超越了模仿学习与强化学习方法
[baseline 数据支持]


## introduction
人类智能的一个独特特征，是能够**无缝融合任务导向的行动与语言推理**（即“内心言语”，Alderson-Day & Fernyhough, 2015）。理论认为，这种能力在人类认知中扮演着关键角色：它支持自我调节与策略规划（Vygotsky, 1987；Luria, 1965；Fernyhough, 2010），并有助于维持工作记忆（Baddeley, 1992）。

在任意两个具体动作之间，我们可能会用语言进行推理
做了什么事/发生什么情况/需要额外信息/ -> 言语推理 ->行动

exp:
追踪进度：“现在所有食材都切好了，我该把水烧开了”；
处理异常或调整计划：“我没有盐了，那就用酱油和胡椒代替吧”；
意识到需要外部信息：“面团该怎么准备？我上网查一下”。

近期研究暗示，这种推理与行动的结合也可能应用于自主系统
但现在的CoT推理是一个静态黑箱：模型仅依赖其内部表征生成思维，未与外部世界建立联系，因此难以进行反应式推理或更新知识，容易导致事实幻觉（hallucination） 和推理过程中的错误传播

![alt text](image.png)





# Noun explanation && Extensive knowledge 
## 推理能力 & 行动能力




# 思考？


